# 反欺诈

## 一：反欺诈四大维度

​	网络及设备终端、用户行为信息、业务事件频次以及欺诈网络图谱。

![img](https://i.loli.net/2020/07/27/fRAYBuQ6kph3j5m.png)

反欺诈中可用到多种社交网络算法：

- 指标：degree, closeness centrality, betweenness centrality, cluster coefficient, triangle count, connected components

- 算法：

  1. PageRank
  2. 社区发现：GN，FastUnfolding, LPA, SLPA , WalkTrap

- 优势：

  1. 客户一度、二度关系是否触黑
  2. 客户消费关联商家是否异常
  3. 一机多人
  4. 识别组团欺诈

  ![image-20200727225030936](https://i.loli.net/2020/07/27/FZdc78SJDbUGTza.png)

![](https://i.loli.net/2020/07/27/FZdc78SJDbUGTza.png)

## 1. 网络及设备终端

为了对抗上面所提到的种种作弊行为，在实践中，我们通常按下面的方法来构造一个设备指纹系统：1）根据收集到的历史数据，形成操作系统签名数据库。这个数据库被用于判断一个设备的操作系统签名是否和声明的操作系统及厂商，型号等吻合，进而判断设备是否是脚本程序或模拟器伪装，是否使用了代理或VPN，是否篡改了UA等设备信息等等；2）对设备的IP来源进行分析，看是否有可疑行为的历史，是否来自机房，从而判断其自动化或作弊的可能性；3）根据设备相关的三十多个特征将设备进行关联，赋予其唯一的身份识别符，此ID是上层特征，即用户行为、频次及欺诈网络图谱的基础。

如何加工这些特征？

## 2. 用户行为信息

在坚实的设备指纹的基础上，需要在会话和账号两层采集和提取用户行为信息。在会话的层面上，借助基于概率的聚类模型和模式挖掘算法(sequential pattern mining)，将用户的行为模式，比如事件发生的次序以及事件发生的间隔时间，归为几类。并在此基础上识别出异常行为模式。这些标示特征为区分正常用户和欺诈者或自动化工具提供了重要信息；更进一步，在账号的层面上：首先以账号为索引，将会话层面上提取到的行为信息特征按时间串联起来，得到账户层面的异常行为标示特征。其次根据账号相关联的历史行为数据，提取出用户的偏好属性，比如是否为僵尸账号，相邻登录的平均地理距离等。最后，我们将这些信息综合起来，形成特有的用于反欺诈的用户画像。

当一个账号再次出现在业务中时，用户画像中的特征就可以帮助我们评估对应业务事件的风险。在实践中，我们还发现，由“羊毛党”控制的账号，通常具有某些相似性，比如所用手机号码都来自某个号段，用户名都由三个小写字母，五个大写字母和四个数字组成。据此，我们就可以定义账号之间的相似度。这样即使一个账号首次出现，我们也可以使用用户画像，对其风险做一个大致的评估。

用户画像除了本身能直接应用于欺诈行为的判断外。还可作为网络图谱模型的输入，为欺诈网络的发现提供线索和依据。

## **3. 业务事件频次特征**

典型的薅羊毛行为的特征表现为短时期，小欺诈额，高频次。从业务角度上来看：由于薅羊毛的欺诈行为有别于正常用户的行为模式，会引起业务事件在某些颗粒度的时间+空间上分布异常。从反欺诈的防范要求角度上来看：需要对异常事件能快速响应，这就要求对客户行为做实时或者近实时的统计、计算。并且综合时间序列分析，个体差异分析以及当前趋势分析这三方面的评估，实时并动态地对异常频次数做标记。

在反优惠套利欺诈的场景里面，我们常常希望在某细颗粒度上制定检测规则：比如某个地域／IP网段每小时交易超过多少为异常。也就是说在时间维度的基础上加入空间维度。这是因为通常来说，欺诈行为会引起在某个空间细颗粒度上，时间分布曲线异常；但不会引起整体层面上的异常，也就是说异常数据会被正常数据淹没掉；而且，整体层面上的模型是无法区分细颗粒度上的差别。换句话说，它只能笼统地告诉我们，数据在某个时段出现了异常，但无法告诉我们在这个时段，按空间维度分比如城市，哪些是异常的，哪些依然是正常的。

在时间维度上加入空间维度，面临两方面的困难：一方面，细颗粒度的维度过高，以网段举例，数据中出现的网段有近十万个，分别建模成本过高；另一方面，细颗粒上的数据通常累积量不够，难以满足时间序列模型所需的条件。通过反复建模实践，我们发现基于贝叶斯框架的生成式模型能较完美地克服上述困难，完成建模目标。由于这类模型综合考虑了事件的时间和空间分布，我们不妨叫它“时空动能模型”。

![img](https://i.loli.net/2020/07/27/ZcTsvE5DIwz63q2.png)

## **4. 欺诈网络图谱**

羊毛党经常通过网络发起组织，在一些单点特征上，同正常用户一样呈现出分散的特点，使得单点特征防御难以奏效。但如果将用户行为用网络的形式建模展示，会发现在一些特殊的图形特征上，欺诈行为明显异于正常行为。实践中，我们借助于图学习(Graph Learning)的一些模型，发现网络图谱模型对于羊毛党的发现特别有效。当然欺诈网络发现依赖于设备指纹以及建立在其基础上的同人模型。

![img](https://i.loli.net/2020/07/27/j8YCIEVOWPMdSx3.png)

在上面的网络图谱里面，红色点表示不同设备；绿色点表示不同账号；蓝色点表示不同APP。红色点和绿色点之间的虚线表示：该设备注册了这个账号；蓝色点与绿色点之间的虚线表示该账号领取了这个APP的某些奖励。

上面的左图是正常用户的行为网络图谱：不同设备的行为是分散的，不一致的。而右边是“公会式羊毛党”的行为网络图谱。在网络图谱上，羊毛党的设备行为会呈现出高度的一致性和集中性。

以上，我们对网络及设备终端、用户行为信息、业务事件频次，欺诈网络图谱这四大维度的特征工程做了一个简单的介绍。在反欺诈实践中，这些提取出来的特征，结合多层动态模型，能有效地识别出高风险薅羊毛行为，帮助企业打击“羊毛党”。

## **二、电商反作弊的基本原则**

 既然很多作弊都是程序自动化完成的，那么就应该尝试做人机识别。由于作弊手段是多样的，所以防御措施也没有一劳永逸的好事，但是可以遵循的一个基本防御原则——提升作弊成本：

**2.1.验证码**

图形验证码是最简单、粗暴的人机识别手段，一旦有了验证码，羊毛党薅羊毛就需增加图片OCR的技术，提升了他们的作弊成本。但以目前的OCR技术水平，图形验证码早已形同虚设。为此，网易云安全(易盾)研发了各种新型验证码，如拖条、拼图、文字点选等智能验证码，在人机识别和用户体验上都得到了很多用户和客户的认可。

**2.2.手机短信验证**

虽然有打码平台的存在，但是接入也需要成本，并且是按手机号码个数计费，成本也不低。提升了作弊的成本，羊毛党自然的会去找成本更低，更容易的地方薅羊毛了。

**2.3. IP规则**

IP也是有限的资源，虽然有很多代理售卖，但也需要成本。IP高频限制，可以作为最基本的防御措施。

**2.4.设备ID/浏览器指纹**

 利用设备特征生成唯一的、稳定的设备ID和浏览器指纹，并基于此做高频限制和统计分析，是非常有效的反作弊手段。但如何获取到真实的设备信息(不是篡改之后的)，以及确保设备信息在传输过程中不被篡改和伪造则需要安全、专业的技术方案，并且还需要长期的安全对抗和技术积累。网易云安全(易盾)依托多年的反作弊经验，推出了专业的、安全可靠的设备ID和浏览器指纹算法，并将其使用于企业客户的反作弊服务中。

目前考拉的反作弊系统对于风险订单的识别主要基于规则引擎，同时结合用户画像评分、关联网络模型和业务名单库等检测手段。

![img](https://i.loli.net/2020/07/27/AkoI9cb784xJGiN.jpg)

规则引擎根据规则条件实时抓取有作弊特征的订单，然而，不是所有满足规则条件的订单都是有问题的，如何将其中的正常用户订单剔除呢?这时就需要使用用户画像评分模型、关联网络模型和业务名单库来提高结果的准确性。 

**1、规则引擎**

规则引擎支持对规则的动态配置和实时统计，系统可以按照不同的时间窗口，在线统计各订单所符合的规则特征的情况，并实时返回结果。规则系统可以输出的有两种结果：

a、订单数据经过所有的规则检测，将各命中规则的分数累加，计算出总分数输出;

b、另一种情形是，取命中规则中等级最高的结果。

**2、用户画像**

这里的用户画像主要是在囤货行为上的用户画像评分，区别于普通平台对用户的综合信用评分。我们选取了几个维度，如用户购买的商品类目数、活跃度、毛利贡献数、历史恶意行为、常用设备等，利用统计方法给用户得出综合分数，再给分值的区间定级，这样就得到所有用户在囤货行为上的画像评分，这个模块的加入可以很大程度地提高反作弊系统的准确性。

**3、关联网络模型**

我们结合无监督学习+有监督学习方法来发掘羊毛党团伙作案的网络模型。首先，我们考察用户在一段时间内所有的订单相关数据的关系链，这些关系链构成一个总网络。接着，搜索网络中的所有子网络，进行连通图分割。再遍历每个子网络，获取网络标签，挖掘网络特征，最后，我们通过机器学习构建一个识别羊毛党的网络模型，下图就是一个典型的关联网络图。

![img](https://i.loli.net/2020/07/27/mFlpVTWr43YbdZJ.jpg)

网络模型判定的结果还可以和规则条件相结合，在不同的业务场景下灵活选择最匹配的风险判定结果，最大程度满足各业务场景的反作弊需求。

**4、名单库**

反作弊系统的名单库细分成很多个类型，取决于各业务场景的需要。比如，今年大火的拼团促销形式在很多电商平台开展，活动对于团长和团员的购买限制是不同的，这就导致团长囤货和团员囤货在订单行为模式上表现得不一样。若要定制拼团活动的黑名单，那就应该对两者做区分，以确保业务的黑名单准确无误伤。

除了黑白名单，还会有灰名单，这些灰名单用户也许在某些大利益点活动时无法参与，将风险拦截在前端，以保障重大活动平稳顺利进行。



## 三、用户账号异常分析

以下，我们将利用统计规律和机器学习的原理，通过FEA（有限元分析，没找到是啥东西:sweat:）建立相应的数据模型，来分析异常账号的异常情况。​​

#### 一、对账号相关数据建模

对历史数据进行分析和学习，刻画和建立**正常行为模型**。一般采用时间序列和马尔柯夫过程等方法。分析

- **帐号的访问频率**
- **在线持续时间**
- **常用的登录时间段**
- **特定内容的访问数据量等因素**

正常模型建立好以后，可以**分析检测用户实际活动与正常模型偏离度，是否在一定的阈值之内，对用户的行为进行决策推断，发现行为是否有异常。**

1、访问频率的模型

根据历史登录数据，结合相关的因素，建立时间序列模型。

2、活跃程度模型

根据用户常用在线时间段，在线时长，活跃程度等建立模型。

3、敏感数据访问量模型

基于时间序列的敏感数据访问情况，如用户访问svn服务器，下载代码的情况，重要数据的修改上传情况等建立时间序列模型。

#### 二、对帐号的特征进行画像

根据各种审计日志，主机日志，数据流信息，分析出过去**常用的ip**，**常用工具，地理位置**等使用环境情况，从不同的角度对用户进行勾画，以确定其基本轮廓。

1、基本要素

帐号名称、常用ip、所在城市、常用浏览器、常用的软件客户端、登录频率、活跃程度、访问协议、常用访问时间段。

2、**动态更新**

随着时间的变化，用户环境的变化，可能用户的行为有很大变化，原有画像有可能失效，就需要分析修正模型，并更新画像，需要有合理的判别更新的机制，提高实际应用中的准确性。

#### 三、基于帐号的关联分析

1、业务的前后关联

实际业务中，很多用户的操作习惯存在前后关联的情况，如先用ssh或远程桌面帐号登录服务器进行一些操作，生成文件，然后用ftp,sftp帐号下载文件。

业务系统的设计逻辑也会使不同帐号业务之间存在前后序列关系， 如用http帐号访问web网站，会触发网站通过一个帐号访问后台数据库，这种业务操作之间存在关联。通过Apriori等算法，分析帐号业务操作之间的关系。

2、同帐号异地多ip， 同ip多帐号的分析

通过大量数据分析，同一个ip有多个同类型的帐号登录，公用帐号使用，异地登陆等很容易发现问题。如，**一个帐号先在北京登录，5分钟后在成都登录，密码泄露的可能性较大。**

3、帐号群体划分

通过对帐号进行相似度计算和聚类分析，对帐号群体进行划分，划分成不同的帐号簇群。分析容易出现异常情况的簇群，更有利于综合得出个体与群体的关系，更好地分析是用户个体行为的变化还是用户群体行为的变化。

## 四、数据埋点

所谓埋点就是在应用中特定的流程收集一些信息，用来跟踪应用使用的状况，后续用来进一步优化产品或是提供运营的数据支撑，包括访问数（Visits），访客数（Visitor），停留时长（Time On Site），页面浏览数（Page Views）和跳出率（Bounce Rate）。这样的信息收集可以大致分为两种：

**页面统计**（track this virtual page view）

**统计操作行为**（track this button by an event）。 

1、数据埋点的方式

- 第一种：自己公司研发在产品中注入代码统计，并搭建起相应的后台查询。
- 第二种：第三方统计工具，如友盟、神策、Talkingdata、GrowingIO等。

如果是产品早期，通常会使用第二种方式来采集数据，并直接使用第三方分析工具进行基本的分析。而对于那些对数据安全比较重视，业务又相对复杂的公司则通常是使用第一种方式采集数据，并搭建相应的数据产品实现其数据应用或是分析的诉求。

2、关键指标

我们先看看无论是APP，H5还是小程序都会关注的指标，了解这些指标的计算方法的细微差异以及复杂性，换个角度来思考埋点的意义。

- **访问与访客**

访问次数（Visits）与访问人数（Vistors）是几乎所有应用都需要统计的指标，这也是最基础的指标。

对于应用的统计来说，经常看到的DAU，MAU，UV等指标都是指统计访客（Vistors）。访问（Visits）是指会话层，用户打开应用花一段时间浏览又离开，从指标定义（访问次数）来说这被称之为统计会话（Session）数。

一次会话（Session 或 Visit）是打开应用的第一个请求（打开应用）和最后一个请求决定的。如果用户打开应用然后放下手机或是离开电脑，并在接下来30分钟内没有任何动作，此次会话自动结束，通常也算作一次访问或会话期（30分钟是早起网页版应用约定俗成的会话数定义，目前用户停留在应用的时长变长，30分钟的限定也可能随之不同，总之是能代表一次用户访问的时长）。

在计算访问人数（Vistors）时，埋点上报的数据是**尽可能接近真实**访客的人数。对于有需要统计独立访客这个指标的场景，这里还是需要强调一下，访问人数（Vistors）并不是真实独立的人，因此收集数据时必须知道访问人数虽然能够很好的反映使用应用的真实访问者的数量，但不等于使用应用的真实人数。（原因是，重复安装的应用，或是手机参数被修改都会使得独立访客的指标收到影响。计算访问人数的埋点都是依赖Cookie，用户打开应用，应用都会在此人的终端创建一个独立Cookie, Cookie会被保留，但还是难免会被用户手动清理或是Cookie被禁用导致同一用户使用应用Cookie不一致，所以独立访客只能**高度接近于**使用应用的真实人数。）

- **停留时长**

停留时长用来衡量用户在应用的某一个页面或是一次访问（会话）所停留的时间。

页面停留时长，表示在每个页面所花费的时间；例如：首页就是进入首页（10：00）到离开首页进入下一个页面(10:01)的时长，首页停留时长计算为1分钟。页面A是2分钟。停留时长的数据并不都是一定采集得到的，比如**页面B**进入时间（10：03），离开出现异常或是退出时间没有记录，这时候计算就是0 （所以指标计算时需要了解埋点的状况，剔除这样的无效数据）。

应用的停留时长，表示一次访问（会话）所停留的时间，计算起来就是所有页面的访问时长，同样是上一个流程，应用的停留时长就是4分钟。

- **跳出率**

跳出率的计算方法现在在各个公司还是很多种，最经常被使用的是：用户只访问了一个页面所占的会话比例（原因是：假设这种场景，用户来了访问了一个页面就离开了，想想用户使用的心里画面应该是：打开应用，心想什么鬼，然后关闭应用甚至卸载了。这个场景多可怕，这也是为什么跳出率指标被如此关注）

跳出率可以分解到两个层次：一是整个应用的跳出率，二是重点的着陆页的跳出率，甚至是搜索关键词的跳出率。跳出率的指标可操作性非常强，通过统计跳出率可以直接发现页面的问题发现关键词的问题。

- **退出率**

退出率是针对页面的，这个指标的目标很简单，就是在针对某个页面有多少用户离开了应用，主要用户反映用户从应用离开的情况。哪些页面需要被改进最快的方式被发掘。（**注意：退出率高不一定是坏事。**例如：预测流程的最终节点的退出率就应该是高的）

- **转化率**

我们在产品上投入这么多，不就是为了衡量产出么？所以对于电商类应用，还有比转化率更值得关注的指标吗？转化率的计算方法是某种产出除以独立访客或是访问量，对于电商产品来说，就是提交订单用户数除以独立访客。

转化率的计算看起来想到那简单，但却是埋点中最贴近业务的数据收集。这也是最体现埋点技巧的指标，需要结合业务特点制定计算方法。提交订单量/访客数是最基本的转化率，转化率还可以分层次，指定用户路径的，如：完成某条路径的提交订单数/访客数。

**试着找一条路径，想想转化率的数据怎么得来的吧，埋点都收集了什么样的数据吧？**

- **参与度**

参与度并不是一个指标，而是一系列的指标的统称，例如访问深度，访问频次，针对电商的下单次数，针对内容服务商的播放次数，及用户行为序列这些都可以是衡量参与度的指标。之所以把参与度列为一个指标，是希望大家明白把指标结合业务，产生化学反应，活学活用去发现事物的本质。

3、埋点的内容

看完关键的这些指标后，其实埋点大致分为两部分，

一部分是统计应用页面访问情况，即页面统计，随页面访问动作发生时进行上报；

另外一部分是统计应用内的操作行为，在页面中操作时进行上报（例如：组件曝光时，组件点击时，上滑，下滑时）。

为了统计到所需要的指标，应用中的所有页面，事件都被唯一标记，用户的信息，设备的信息，时间参数以及符合业务需要的参数具体内容被附加上报，就是埋点。

4、关于埋点的数据的注意事项

不要过分追求完美。埋点是为了更好地使用数据，不要试图得到精准的数据要得到的是高质量的埋点数据，前面讨论跳出率就是这个例子，得到能得到的数据，用不完美的数据来达成下一步的行动，追求的是高质量而不是精确。这是很多数据产品容易入坑的地，要经常提醒自己。

## 五、反欺诈模型设计

#### 维度1

通过社交 （同学圈，同事圈，亲戚圈），手机通讯录 评判

#### 维度2

##### 1 从用户申请提交的数据层面

· 1.1年龄和学历与收入不符合。
· 1.2通讯录无直属亲人。
· 1.3拥有资产如车、房等与居住地址 或消费水平不符合。
· 1.4现住地址与公司地址差异较大。如不在同一城市中。
· 1.5QQ或APP最近常登陆地区不在现 住地址或公司地址中。
· 1.6最近手机通讯地址不在常用地址 中
· 1.7收货信息上手机号码与通讯录上 手机号码反差较大

##### 2 申请的记录在已有的记录有类似出现

· 2.1电话号码出现在已有放贷或申请记录中
· 2.2身份证号码出现在已有放贷或申请记录中
· 2.3地址出现在已有放贷或申请记录中
· 2.4QQ号码在已有放贷或申请记录中
· 2.5同一邮箱已有放贷或申请记录
· 2.6银行卡号在已有放贷或申请记录中
· 2.7同一车牌号在已有放贷或申请记录中

##### 3 黑名单 征信

· 3.1 贷联盟公布黑名单
· 3.2 人行征信黑名单
· 3.3 征信是否有逾期

##### 4 操作

· 4.1 单个IP（设备等）,多次申请账号 
· 4.2 同一账号,短时间登陆多个地域相差大的IP
· 4.3 单个IP（设备等），多次申请贷款
· 4.5 同一账号,短时间内申请贷款数量或金额超过一定限制
· 4.6 同一关系圈,出现相似借贷

另一种获取数据层面分类：

1. 账户数据，如账户名、手机号、银行卡号、姓名、年龄、性别、身份证、地址等，这类信息主要提供了用户的基础属性

2. 业务数据，主要收集业务场景下的数据，贷款业务有贷款维度的属性，优惠券兑换有优惠券的信息，在对这部分事件做风控时，业务数据是核心关键数据

3. 设备信息，在风控过程中，还需要对用户的常用设备指纹，环境信息进行收集

4. 用户轨迹，主要包括用户的访问顺序，操作频率，按键信息等

这边列举下风控领域的用户画像标签：如是否曾经被盗、常用设备、常用地、消费偏好、所属企业画像、是否有危险伙伴往来、手机画像、IP画像、社工账号等。在上述标签中，不难发现像手机画像，IP画像可能并不是直接从数据收集中获取，这类信息很多时候我们是通过另外的技术手段进行的收集工作，如手机是否为猫池号，是否为小号空号，如IP是否为VPN、代理、是否属于爬虫等。

参考资料：

- [数据埋点是什么](https://www.zhihu.com/question/36411025/answer/147581103)
- [用户账号异常分析](https://blog.csdn.net/u013129109/article/details/79953310)

