# 机器学习常用损失函数小结

机器学习中的监督学习本质上是给定一系列训练样本$(x_i,y_i)$ ，尝试学习$x\rightarrow y$的映射关系，使得给定一个 $x$，即便这个$x$不在训练样本中，也能够得到尽量接近真实$y$的输出$\hat y$ 。而损失函数（Loss Function）则是这个过程中关键的一个组成部分，用来**衡量模型的输出$\hat y$ 与真实的$y$ ** **之间的差距**，给模型的优化指明方向。

本文将介绍机器学习、深度学习中分类与回归常用的几种损失函数，包括均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss、交叉熵损失函数 Cross Entropy Loss、Hinge 损失 Hinge Loss。主要介绍各种损失函数的基本形式、原理、特点等方面。

### 目录

1. 前言
2. 均方差损失 Mean Squared Error Loss
3. 平均绝对误差损失 Mean Absolute Error Loss
4. Huber Loss
5. 分位数损失 Quantile Loss
6. 交叉熵损失 Cross Entropy Loss
7. 合页损失 Hinge Loss
8. 总结

## 均方差损失 Mean Squared Error Loss

均方差 [Mean Squared Error (MSE)](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mean_squared_error) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：

![img](https://i.loli.net/2020/08/29/wF9lET12Hzat4qk.jpg)

可以看到这个实际上就是均方差损失的形式。也就是说**在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的**，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。



## 平均绝对误差损失 Mean Absolute Error Loss

### 基本形式与原理

平均绝对误差 [Mean Absolute Error (MAE)](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mean_absolute_error) 是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下

![img](https://i.loli.net/2020/08/29/ohxLjeNvk573BCA.jpg)

### MAE 与 MSE 区别

MAE 和 MSE 作为损失函数的主要区别是：MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮，即更加不易受到 outlier 影响。

**MSE 通常比 MAE 可以更快地收敛**。当使用梯度下降算法时，MSE 损失的梯度为$-\hat{y_i}$，而 MAE 损失的梯度为$\pm1$，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差$\lvert y_i-\hat{y_i} \rvert$很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因。

**MAE 对于 outlier 更加 robust**。我们可以从两个角度来理解这一点：

- 第一个角度是直观地理解，下图是 MAE 和 MSE 损失画到同一张图里面，由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失。因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响。

![img](https://i.loli.net/2020/08/29/pdT48Mbz96mO7cv.jpg)

- 第二个角度是从两个损失函数的假设出发，MSE 假设了误差服从高斯分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于 outlier 更加 robust。参考下图（来源：[Machine Learning: A Probabilistic Perspective](https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~murphyk/MLbook/) 2.4.3 The Laplace distribution Figure 2.8），当右图右侧出现了 outliers 时，拉普拉斯分布相比高斯分布受到的影响要小很多。因此以拉普拉斯分布为假设的 MAE 对 outlier 比高斯分布为假设的 MSE 更加 robust。

![img](https://i.loli.net/2020/08/29/5RkUC2K6Dqs34xi.jpg)



## Huber Loss

上文我们分别介绍了 MSE 和 MAE 损失以及各自的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，[Huber Loss](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Huber_loss) 则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。其原理很简单，就是在误差接近 0 时使用 MSE，误差较大时使用 MAE，公式为

![image-20200826201434546](https://i.loli.net/2020/08/26/7ewnCvlO9H1osuT.png)

上式中$\delta$ 是 Huber Loss 的一个超参数的，$\delta$ 值是 MSE 和 MAE 两个损失连接的位置。上式等号右边第一项是 MSE 的部分，第二项是 MAE 部分，在 MAE 的部分公式为$\delta \lvert y_i - \hat{y_i}\rvert -\frac{1}{2}\delta^2$是为了保证误差$|y-\hat{y}|=\pm\delta$时 MAE 和 MSE 的取值一致，进而保证 Huber Loss 损失连续可导。

![img](https://i.loli.net/2020/08/29/CXdqG9FxlsNZU7Y.jpg)

### Huber Loss 的特点

Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个$\delta$超参数。



## 分位数损失 Quantile Loss

分位数回归 [Quantile Regression](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Quantile_regression) 是一类在实际应用中非常有用的回归算法，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数。例如我们可以分别拟合出多个分位点，得到一个置信区间，如下图所示：

![img](https://i.loli.net/2020/08/29/Jg8Pz4tU1ObSdf3.jpg)

分位数回归是通过使用分位数损失 Quantile Loss 来实现这一点的，分位数损失形式如下，式中的 r 分位数系数。![image-20200826203709496](https://i.loli.net/2020/08/29/tJwCfXvKZDAIWjs.png)

如何理解这个损失函数呢？这个损失函数是一个分段的函数 ，将$\hat{y_i} \geq y_i$（高估） 和$\hat{y_i} < y_i$（低估） 两种情况分开来，并分别给予不同的系数。当时$r>0.5$，低估的损失要比高估的损失更大，反过来当$r<0.5$时，高估的损失比低估的损失大；分位数损失实现了**分别用不同的系数控制高估和低估的损失，进而实现分位数回归**。特别地，当时$r=0.5$，分位数损失退化为 MAE 损失，从这里可以看出 MAE 损失实际上是分位数损失的一个特例 — 中位数回归（这也可以解释为什么 MAE 损失对 outlier 更鲁棒：MSE 回归期望值，MAE 回归中位数，通常 outlier 对中位数的影响比对期望值的影响小）。![image-20200826204558871](https://i.loli.net/2020/08/26/v2KCSn5UyRfBtzi.png)

下图是取不同的分位点 0.2、0.5、0.6 得到的三个不同的分位损失函数的可视化，可以看到 0.2 和 0.6 在高估和低估两种情况下损失是不同的，而 0.5 实际上就是 MAE。

![img](https://i.loli.net/2020/08/29/4zXVLMbogp5njGB.jpg)



## 交叉熵损失 Cross Entropy Loss

上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 [Cross Entropy Loss](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Cross_entropy)。

### 二分类

下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为 0 时输出不同输出的损失，黄线是目标值为 1 时的损失。可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长。

![img](https://i.loli.net/2020/08/29/kYuxOJ5zpj6TB3Z.jpg)



### 多分类

在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值$y_i$现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 [Softmax](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Softmax_function) 函数。

## 合页损失 Hinge Loss

合页损失 [Hinge Loss](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Hinge_loss) 是另外一种二分类损失函数，适用于 maximum-margin 的分类，支持向量机 Support Vector Machine (SVM) 模型的损失函数本质上就是 Hinge Loss + L2 正则化。合页损失的公式如下![image-20200826204739214](https://i.loli.net/2020/08/26/wIijyBsAxpG15tJ.png)

下图是$y$为正类， 即$sgn(y)=1$时，不同输出的合页损失示意图

![img](https://i.loli.net/2020/08/29/G3vVu6HItQLJhbd.jpg)

$y$可以看到当$y$为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在$(0,1)$区间时还会有一个较小的惩罚。即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失。使用合页损失直觉上理解是要**找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类**。

## 总结

本文针对机器学习中最常用的几种损失函数进行相关介绍，首先是适用于回归的均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss，两者的区别以及两者相结合得到的 Huber Loss，接着是应用于分位数回归的分位数损失 Quantile Loss，表明了平均绝对误差损失实际上是分位数损失的一种特例，在分类场景下，本文讨论了最常用的交叉熵损失函数 Cross Entropy Loss，包括二分类和多分类下的形式，并从信息论的角度解释了交叉熵损失函数，最后简单介绍了应用于 SVM 中的 Hinge 损失 Hinge Loss。本文相关的可视化代码在 [这里](https://link.zhihu.com/?target=https%3A//github.com/borgwang/toys/tree/master/loss_functions)。

------

## 参考资料

- [损失函数小结](https://zhuanlan.zhihu.com/p/77686118)
- [Machine Learning: A Probabilistic Perspective](https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~murphyk/MLbook/)
- [Picking Loss Functions - A comparison between MSE, Cross Entropy, and Hinge Loss](https://link.zhihu.com/?target=https%3A//rohanvarma.me/Loss-Functions/)
- [5 Regression Loss Functions All Machine Learners Should Know](https://link.zhihu.com/?target=https%3A//heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)
- [Quantile Regression Demo](https://link.zhihu.com/?target=https%3A//gist.github.com/borgwang/4313e9375ef233c3b812f9f80f1af2bb)